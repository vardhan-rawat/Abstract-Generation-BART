we introduce a novel semi -supervised neurosymbolic approach for relation extraction that is explainable and requires minimal supervision. We evaluate this approach on the TACRED dataset and obtain competive results in a few -shot setting where the only supervision comes from a small number of example rules. We find that our approach outperforms the model that combines a neural relation classifier with an explanation classifier that identifies context words that are important for the relation at hand (e.g., which extracts the relation per:city_of_birth between John and London in the sentence John was born in London ) in our experiments by 15 F1 points per relation type and by a factor of 10 more than the self -trained model that contains the seed rules. We also find that we perform better than a sister approach that uses the relation classifiers generated from explanations for self supe rvision for information extraction than our approach does and that our method is simpler as it does not require a separate natural language inference component. We discuss two related approaches that are closest to the proposed work: bootstrapping/self -training and recent prompt -based zero - or few - shot methods that are similar to our approach and provide us with the ability to handle tasks with a large amount of parameters with only a few prompts or with few examples or with a few examples with few prompts as a natural language task in a semisupervised state of the art task driven by a small set of manual templates (NLIMs). We discuss how these approaches can be applied to a general knowledge knowledge and explainability task with large pre-trained data sets and how they can be used to augment natural language knowledge in real world applications such as search engines and word embeddings and dictionary engines. we also discuss the potential of these approaches to improve the performance of natural language search engines that are already in use in real -world app lications where annotated data is expensive to obtain and how our approach can be adapted to these real world application scenarios where only a small amount of data is required for the task to be annotated and the task is self supervised and self supervised. we show that the approach is suitable for real world applications where only one or few prompts are required to train the model with the data and that the task can be self supervised in the same way as the previous approach. we discuss the advantages and limitations of both approaches and discuss the possible applications of our approach in the real world and in the context of our recent work on natural language searches.