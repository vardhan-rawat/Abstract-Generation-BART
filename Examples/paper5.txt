we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output positions in sequence modeling and transduction problems such as language modeling and machine t ranslation. The Transformer allows for significantly more parallelization an d can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs on a single P100 card. The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for al l input and al l output positions. In this work we counter a countera ct effect called intra -Head Attention as described in section 3.2.1, where the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions grows linearly for ConvS 2S and logithmically for ByteNet. This makes it mor e difficult to learn between distant positions at effective resolution due to a reduced cost to averaging an effective cost of an attention -weighted effect due to the interdependency between positions in the model and in the training examples. In this paper we show that the transformer can be used to perform simple language modeling tasks on a recurrent network instead of a sequence of order and end-to-end memory networks based on a self -attention mechanism instead of the recurrent model and perform simple question answering tasks such as word comprehension and word comprehension in the context of a single question [34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 45 and 45, 45, 45, 47, 48, 46 and 45, 47, 48, 48, 48, 49, 49, 50 and 50, 50, 51, 52, 50, 52, 53, 54, 56, 56, 57, 54, 56 and 56, 56, 57, 58, 57, 58, 58, 58, 59, 59, 59, 60, 62, 62, 62, 63, 62, 63, 63, 63, 64, 63 and 63, 62 and 63. The transformer is based on the idea that a single attention mechanism is used to compute a representation of the order of the sequence of words in a single word in a given word and is independent of the position of the word in which the word is in the sequence and is parallelized to the order in which it is ordered in the word and to the position in which that word is placed in the input and position in the order it is in. We also show that this mechanism can be applied to a number of simple task solving tasks including word comprehension, word comprehension, word matching and sentence comprehension and task reading.